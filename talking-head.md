# 🗣️ Talking Head Generation

This page lists the top datasets and models for generating lifelike talking heads using images and audio input.

---

## 🧪 Datasets

- [VoxCeleb2](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox2.html) – Speaker identification videos
- [FaceForensics++](https://github.com/ondyari/FaceForensics) – Deepfake detection and generation
- [TalkingHead-1KH](https://github.com/tcwang0509/TalkingHead-1KH) – Large-scale talking head dataset
- [MEAD](https://github.com/uniBruce/Mead) – Multi-expression audio dataset
- [CelebV-HQ](https://github.com/CelebV-HQ/CelebV-HQ) – High-quality talking head benchmark
- [MultiTalk](https://github.com/postech-ami/MultiTalk) – Multi-view, multi-lingual dataset

---

## 🧠 Key Models

### 📽️ 2024–2025
- [LivePortrait](https://github.com/KwaiVGI/LivePortrait) – Real-time animation with stitching and control
- [HunyuanPortrait](https://github.com/kkakkkka/HunyuanPortrait) – Condition-controlled portrait animation
- [EMOPortraits](https://github.com/neeek2303/EMOPortraits) – Emotion-enhanced avatars
- [X-Portrait](https://arxiv.org/abs/2403.15931) – Motion retargeting with hierarchical attention

### 🎞️ Earlier Models
- [StyleHEAT](https://github.com/FeiiYin/StyleHEAT) – StyleGAN-based one-shot editable face generation
- [MegaPortraits](https://samsunglabs.github.io/MegaPortraits/) – One-shot megapixel avatars
- [DaGAN](https://github.com/harlanhong/CVPR2022-DaGAN) – Depth-aware generation
- [TS-Net](https://github.com/nihaomiao/WACV23_TSNet) – Identity-preserving talking heads

---

💡 **Tip**: Use recent models like LivePortrait and EMOPortraits for real-time, emotion-aware avatar generation in apps.

