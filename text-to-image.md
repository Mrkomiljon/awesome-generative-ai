# 🖼️ Text-to-Image Generation

This section provides a curated list of recent and powerful models that convert natural language into images.

---

## 🎨 Notable Models

### 🔷 [ControlNet](https://github.com/lllyasviel/ControlNet)
- Add structural control (pose, depth, edge) to pre-trained diffusion models
- Works with Stable Diffusion and other backbones

### 🔷 [T2I-Adapter](https://github.com/TencentARC/T2I-Adapter)
- Plug-in modules to guide image synthesis with external conditions

### 🔷 [DreamBooth](https://github.com/XavierXiao/Dreambooth-Stable-Diffusion)
- Fine-tune a diffusion model to generate specific subjects
- Few-shot personalized image generation

### 🔷 [StyleDiffusion](https://github.com/MatthewLWang/StyleDiffusion)
- Text-guided style transfer in diffusion synthesis

### 🔷 [Sana](https://github.com/NVlabs/Sana)
- Scalable personalization in diffusion with multi-subject support

### 🔷 [Infinity](https://github.com/FoundationVision/Infinity)
- Infinite resolution image generation using patch-wise modeling

---

## 🧪 Emerging Ideas

- [IMAG-Dressing](https://github.com/muzishen/IMAGDressing) – Text-driven virtual try-on
- [Text-to-Pose-to-Image](https://github.com/clement-bonnet/text-to-pose) – Improves diffusion with pose guidance
- [Rich-Text-to-Image](https://github.com/songweige/rich-text-to-image) – Enhanced prompt conditioning
- [custom-diffusion](https://github.com/adobe-research/custom-diffusion) – Multi-concept personalization

---

💡 **Tip**: Combine T2I-Adapter or ControlNet with Stable Diffusion for controllable and reliable synthesis.

